##目录
* url每行一个存储在目录文件中
* 目录分为未爬取和已爬取
* 动态爬取太难，部分网站有现成的目录
* 或者搜索界面搜索空白，基本也有几万项了，把链接扒下来做成目录

##日志
* 日志改为了文件写入，可以通过tail命令查看
* 或者删掉setting文件里的LOG-FILE，日志会在命令行输出

##运行
* 进入项目目录，终端输入scrapy crawl {spidername}
* 例如：scrapy crawl springer

##数据库和pipeline
* 参考上届项目

##具体看注释